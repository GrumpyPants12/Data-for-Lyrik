{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1zIbmXazh5r5gnmI_8OS8F1RPQc5hCI-p",
      "authorship_tag": "ABX9TyMnfs/EFWQAK/L9PXcIWJ2Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrumpyPants12/Data-for-Lyrik/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkLmOdAGyaI8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import requests\n"
      ],
      "execution_count": 400,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzF6Q_TJ0VeF"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('Gedichte.txt', 'https://raw.githubusercontent.com/GrumpyPants12/Data-for-Lyrik/main/Gedichte.txt')\n",
        "\n"
      ],
      "execution_count": 401,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLAJ_oSC0zi1",
        "outputId": "c660b6e3-2ea7-45b1-bcef-fd4dd1621a42"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": 402,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 73195 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXjXGBqE03Vc",
        "outputId": "c171f68d-891a-4f68-bfcf-736424135c89"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 403,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html><html><head><meta name=\"google\" content=\"notranslate\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge;\"><style nonce=\"hP1Et2qb95cDcTFb3KbbIw\">@font-face{font-family:'Roboto';font-style:italic;font-weight:400;src:url(//fonts.gstatic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHRJ2-wS09Zk",
        "outputId": "c9b5f57f-8bae-4e37-a846-5e065ac660ad"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": 404,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UX0XlkV3fJ6",
        "outputId": "28b5e9b8-6701-425d-da46-14990d56bb10"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": 405,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 405
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o_RjKp-3iZy"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "ids = ids_from_chars(chars)\n",
        "ids\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "chars = chars_from_ids(ids)\n",
        "chars\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 406,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_9oUzXK3_Hi",
        "outputId": "105a48c8-6268-44aa-8cdb-0c799146161b"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 407,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(73195,), dtype=int64, numpy=array([30,  3, 38, ..., 79, 78, 32])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 407
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEhgTlqY4AYy",
        "outputId": "ee219998-bd4e-424b-9b7f-2c65680b6aad"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))\n",
        "\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": 408,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<\n",
            "!\n",
            "D\n",
            "O\n",
            "C\n",
            "T\n",
            "Y\n",
            "P\n",
            "E\n",
            " \n",
            "tf.Tensor(\n",
            "[b'<' b'!' b'D' b'O' b'C' b'T' b'Y' b'P' b'E' b' ' b'h' b't' b'm' b'l'\n",
            " b'>' b'<' b'h' b't' b'm' b'l' b'>' b'<' b'h' b'e' b'a' b'd' b'>' b'<'\n",
            " b'm' b'e' b't' b'a' b' ' b'n' b'a' b'm' b'e' b'=' b'\"' b'g' b'o' b'o'\n",
            " b'g' b'l' b'e' b'\"' b' ' b'c' b'o' b'n' b't' b'e' b'n' b't' b'=' b'\"'\n",
            " b'n' b'o' b't' b'r' b'a' b'n' b's' b'l' b'a' b't' b'e' b'\"' b'>' b'<'\n",
            " b'm' b'e' b't' b'a' b' ' b'h' b't' b't' b'p' b'-' b'e' b'q' b'u' b'i'\n",
            " b'v' b'=' b'\"' b'X' b'-' b'U' b'A' b'-' b'C' b'o' b'm' b'p' b'a' b't'\n",
            " b'i' b'b' b'l'], shape=(101,), dtype=string)\n",
            "b'<!DOCTYPE html><html><head><meta name=\"google\" content=\"notranslate\"><meta http-equiv=\"X-UA-Compatibl'\n",
            "b'e\" content=\"IE=edge;\"><style nonce=\"hP1Et2qb95cDcTFb3KbbIw\">@font-face{font-family:\\'Roboto\\';font-styl'\n",
            "b'e:italic;font-weight:400;src:url(//fonts.gstatic.com/s/roboto/v18/KFOkCnqEu92Fr1Mu51xIIzc.ttf)format('\n",
            "b\"'truetype');}@font-face{font-family:'Roboto';font-style:normal;font-weight:300;src:url(//fonts.gstati\"\n",
            "b\"c.com/s/roboto/v18/KFOlCnqEu92Fr1MmSU5fBBc9.ttf)format('truetype');}@font-face{font-family:'Roboto';f\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrQ3ujT64SxL",
        "outputId": "6682d15c-21ef-46dc-a40a-bdde365bcd14"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "split_input_target(list(\"Tensorflow\"))\n"
      ],
      "execution_count": 409,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 409
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UmkZaJp4Xz6",
        "outputId": "33b27a55-c96d-4fe4-af5a-942cae8c93ad"
      },
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 410,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'<!DOCTYPE html><html><head><meta name=\"google\" content=\"notranslate\"><meta http-equiv=\"X-UA-Compatib'\n",
            "Target: b'!DOCTYPE html><html><head><meta name=\"google\" content=\"notranslate\"><meta http-equiv=\"X-UA-Compatibl'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J6e0Fyh4a6q",
        "outputId": "6f263578-90cf-451e-83d7-99c86f20c0c2"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 411
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hshQzTwv4egU"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 412,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ot_zBxW4hdE",
        "outputId": "5ff24b98-0c97-42b6-986f-9e731b18deba"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 413,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 96) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkq1nypS4nfC",
        "outputId": "5aff3d89-d2f1-411e-8370-e7f20d588e9c"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices\n"
      ],
      "execution_count": 414,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([39, 63,  1, 30, 90, 20, 39, 51,  8, 22, 70, 48, 75, 46, 58, 18, 47,\n",
              "       59, 56, 35, 67, 26, 20, 74, 41, 51, 53, 74, 17, 28, 83, 20, 57, 91,\n",
              "       45, 33, 82, 20, 84,  1, 95, 94, 81,  3, 39, 87, 53, 46, 48,  6, 18,\n",
              "       61,  9, 72, 31, 26, 92, 25, 59, 68, 20, 42, 93, 85, 72, 66, 28, 47,\n",
              "       81, 34, 20, 31,  0, 29, 93, 31, 69, 90, 60, 77, 74, 65, 19, 13, 17,\n",
              "       58,  9, 28,  2, 34, 31, 93, 69, 90,  8, 69, 50,  8, 46, 15])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 414
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS9kGk2c4tTS",
        "outputId": "b26f4d1b-0cc4-44e1-b6a5-dabf9ec2845d"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 415,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b' a([[l,2],[m,3]]);if(2!=q.get(l)||3!=q.get(m))return!1;q.delete(l);q.set(m,4);return!q.has(l)&&4==q.'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"E]\\n<x2EQ&4dNiLX0MYVAa82hGQSh/:q2WyK?p2r\\n}|o!EuSLN$0['f=8z7Yb2H{sf`:Mo@2=[UNK];{=cxZkh_1+/X': @={cx&cP&L-\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcbexnM04zwa",
        "outputId": "f9be7cd3-e933-41bc-ccb4-c608d7427a5d"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 416,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 96)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.564741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmWGLFZS46Ky"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 417,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNoj4dVB48u7"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 418,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUrvOawN49tj"
      },
      "source": [
        "EPOCHS = 1\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ezd161KADMZ"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 368,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2STlgY0AH86",
        "outputId": "558c3e4f-d7f3-405a-82d7-ca80f4592694"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 369,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "O, call my fearer a taping go.\n",
            "\n",
            "YORK:\n",
            "No were as verfealy of Nirst,\n",
            "The steal is biin, dear with a mart for that Richard is't for I may:\n",
            "The gearous sucher, make most once saw his dest.\n",
            "\n",
            "LURIO:\n",
            "How you slay! the order-partaly tenter tongue\n",
            "Alpossing in their boints with the thronk'st?\n",
            "The aufter of Isabel, one:\n",
            "none widow it call your life to so\n",
            "ere you har lives.\n",
            "Meaving munder than 'He 'Seakent! a please;\n",
            "Candlumable that houress doth the end.\n",
            "\n",
            "PETRUCHIO:\n",
            "Camillo, a bact disparts! is the dear man were I morning in Pray.\n",
            "\n",
            "MENENIUS:\n",
            "Now he blessed life, that\n",
            "I have best of seenche's black.\n",
            "\n",
            "WARWICK:\n",
            "Even you will be it: Oxf made you, my lord, in my hear\n",
            "Where you would the lebusy of take o' the mostive, mad,\n",
            "All my matter than my thing brother's bastle, Till I be matter't\n",
            "the clom. kill behind;\n",
            "Her offen my whilh I will him,\n",
            "Ay, bit me gown or untruct seffict Hartion,\n",
            "The custitu mender of the Lesseng off you\n",
            "All give you go unword,' live,\n",
            "Beunt the bidder wiet out of used this a lie? \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.875859260559082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_Puze4B6wJt"
      },
      "source": [
        "# Neuer Abschnitt"
      ]
    }
  ]
}